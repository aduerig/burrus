papers:
- AGZ paper: https://github.com/pinae/TensorFlow-MNIST-example/blob/master/fully-connected.py


-implement montecarlo search in c++ (again probably just as a player, need to read papers again to see exact format)
	- no hard rollouts
	- Dirichlet noise Dir(α) was added to the prior probabilities in the root node; this was scaled in inverse proportion to the approximate number of legal moves in a typical position, to a value of α = {0.3, 0.15, 0.03} for chess, shogi and Go respectively.
	- 800-1000 sims per move
	- only one agent


-set up tensorflow model in python, maybe think of an architecture (im thinking like 10-15 residual convolutional layers)
	-maybe recompile tensorflow from source for speedier avx intructions (may be 3-4x speedup)
		-https://github.com/tensorflow/tensorflow/issues/7778
	- https://github.com/AppliedDataSciencePartners/DeepReinforcementLearning
	- cross entropy on policy and squared mean on value
	- good comments: https://www.reddit.com/r/MachineLearning/comments/76xjb5/ama_we_are_david_silver_and_julian_schrittwieser/
	- ok talk, not too much, but silver is speaking https://www.youtube.com/watch?v=Wujy7OzvdJk
	- our idea (model stemmed from agz_unformatted_nature.pdf (mastering the game of go wthout human knowledge))
		- one convolutional block followed by 10 residual blocks (orig paper is 19-39 resid blocks)
		- conv bloc is:
			- 128 filters of kernal size 3x3 with stride 1 (orig paper is 256)
			- batch norm
			- ReLU
		- residual block is (pretty similar to conv block):
			- 128 filters of kernal size 3x3 with stride 1 (orig paper is 256)
			- batch norm
			- ReLU
			- 128 filters of kernal size 3x3 with stride 1 (orig paper is 256)
			- batch norm
			- skip connection that adds input to block
			- ReLU
		- passed into two "heads":
			- policy head:
				- convolution of 2 filters of kernal size 1x1 with stride 1 (used as a dimensionality reduction for the move space)
				- batch norm
				- ReLU
				- FC layer that outputs move space (in logit probabilitites)
			- value head
				- convolution of 1 filters of kernal size 1x1 with stride 1 (used as a dimensionality reduction for the move space)
				- batch norm
				- ReLU
				- FC layer that outputs to hidden layer of size 256
				- ReLU
				- FC layer that outputs to scaler
				- tanh non-linearity [-1, 1]
	- use L2 squared regularization
		c is set to 10^-4 (in the l2 term)
	- checkpoint is every 1000 steps
	- how to optimize multiple loss functions in tensorflow?
		- they do it in keras here: https://github.com/AppliedDataSciencePartners/DeepReinforcementLearning/blob/master/model.py
	- i believe skip connections is a simple add, nothing more
	- minibatch size is 2048 (split up as 32 per worker in paper)
	- SGD with momentum and learning rate annealing
		- "The learning rate was set to 0.2 for each game, and was dropped three times (to 0.02, 0.002 and 0.0002" - from AZ paper (slightly diff than AGZ)
	- "700,000 steps (mini-batches of size 4,096)" - from AZ (slightly diff than AGZ)
	- updated continually (probably new model after 1k steps?)



-write script to port trained python model to c++ to be ru n
	- running trained model is called infrence
		- https://medium.com/@hamedmp/exporting-trained-tensorflow-models-to-c-the-right-way-cf24b609d183 
			- about a year out of date
		- https://stackoverflow.com/questions/35508866/tensorflow-different-ways-to-export-and-run-graph-in-c/43639305#43639305
			- very promising (but also a bit out of date)
		- http://www.bitbionic.com/2017/08/18/run-your-keras-models-in-c-tensorflow/
		- http://jackytung8085.blogspot.com/2016/06/loading-tensorflow-graph-with-c-api-by.html

-MPI to run games in parallel for data collection (probably just one processor to collect all data, the game data is small)

-script to automate flow of train agent -> script to port trained model to c++ -> MPI data collection -> repeat
	- agent should just be initialized with random weights
	- "The sole exception is the noise that is added to the prior policy to ensure exploration (29); this is scaled in proportion to the typical number of legal moves for that game type." - from AZ paper
	- AlphaGo Zero paper uses "25000 games per iteration, 1600 simulations per turn"
	- temperature? (quoted in alphagozero paper)